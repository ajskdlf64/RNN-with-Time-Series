{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 순환 신경망으로 시퀀스 데이터 모델링"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index\n",
    "\n",
    " - **1. 시퀀스 데이터란?**\n",
    "<br><br>\n",
    " - **2. 시퀀스 모델링을 위한 RNN**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 시퀀스 데이터란?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 시퀀스 데이터 모델링 : 순서를 고려한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다른 데이터 타입과 다르게 **시퀀스(sequence)**는 특별하다. 시퀀스 원소들은 특정 순서를 가지므로 상호 독립적이지 않기 때문이다.\n",
    "<br><br>\n",
    "일반적으로 지도 학습의 머신 러닝 알고리즘은 입력 데이터가 **독립동일분포(Independent and Identically Distributed, IDD)**라고 가정한다. 예를 들어 $n$개의 데이터 샘플 $x^{(1)}$,$x^{(2)}$,$\\cdot\\cdot\\cdot$,$x^{(n)}$이 있을 때 머신 러닝 알고리즘을 훈련하기 위해 데이터를 사용하는 순서는 상관이 없다.\n",
    "<br><br>\n",
    "시퀀스 데이터를 다룰 때는 더 이상 이런 가정이 유효하지 않다. 시퀀스는 정의 잔체가 순서를 고려한 데이터이기 때문이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 시퀀스 표현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "입력 데이터에서 의미 있는 순서를 가지도록 시퀀스를 구성한다. 그 다음 머신 러닝 모델이 이런 유용한 정보를 사용하도록 만들어야 한다.\n",
    "<br><br>\n",
    "이제 시퀀스를 ($x^{(1)}$,$x^{(2)}$,$\\cdot\\cdot\\cdot$,$x^{(T)}$)처럼 나타내자. 위의 첨자는 샘플 순서를 나타낸다. $T$는 시퀀스 길이이다. 시퀀스의 좋은 예는 **시계열 데이터(Time Series)**이다. 여기서 각 샘플 포인트 $x^{(t)}$는 특정 시간 $t$에 속한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음은 시계열 데이터의 예이다. $x$와 $y$는 시간축을 따라 순서대로 나열되어 있다. 따라서 $x$와 $y$는 시퀀스 데이터이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![figure](./figure/figure01.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLP와 CNN 같이 기본적인 신경망 모델은 입력 샘플의 순서를 다루지 못한다. 쉽게 생각해서 이런 모델들은 이전에 본 샘플을 기억핮 ㅣ못한다. 샘플이 정방향과 역방향단게를 통과하고 가중치는 샘플이 처리되는 순서와 상관없이 업데이트 된다.\n",
    "<br><br>\n",
    "반면에 RNN은 시퀀스 모델링을 위해 고안되었다. 지난 정보를 기억하고 이를 기반으로 새로운 이벤트를 처리할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 시퀀스 모델링의 종류"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![figure](./figure/figure02.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "어떤 입력 데이터와 출력 데이터가 있다고 가정해보자. 입력과 출력 데이터가 시퀀스로 표현되지 않는다면 일반 데이터로 처리한다. 이런 데이터를 모델링하려면 MLP나 CNN 같은 방법 중 하나를 사용할 수 있다. 입력이나 출력이 시퀀스라면 다음 3가지 중 하나로 구성된다.\n",
    "\n",
    "\n",
    " - **many-to-one** : 입력 데이터가 시퀀스이다. 출력은 시퀀스가 아니라 고정 크기의 벡터이다. 예를 들어 감성 분석에서 입력은 텍스트이고 출력은 클래스 레이블인 경우이다.\n",
    " - **one-to-many** : 입력 데이터가 시퀀스가 아니라 일반적이 형태이다. 출력은 시퀀스이다. 이런 종류의 예로는 이미지 캡셔닝이 있다. 입력이 이미지이고 출력은 영어 문장이다.\n",
    " - **many-to-many** : 입력과 출력이 모두 시퀀스이다. 이런 종류는 입력과 출력이 동기적인지 아닌지에 따라 더 나눌 수 있다. 동기적인 many-to-many 모델링 작업의 예로는 각 프레임이 레이블 되어 있는 비디오 분류이다. 그렇지 않은 many-to-many 모델의 예는 한 언어에서 다른 언어로 번역하는 작업이다. 예를 들어 독일어로 번역하기 전에 전체 영어 문자을 읽어 처리한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 시퀀스 모델링을 위한 RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 RNN 구조와 데이터 흐름 이해"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN 구조를 이해하기 위해 기본 피드포워드 신경망과 RNN을 나란히 살펴보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![figure](./figure/figure03.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "두 네트워크 모두 하나의 은닉층만이 있다. 가ㅣ본 피드포워드 네트워크에서 정보는 입력에서 은닉층으로 흔 후 은닉층에서 출력층으로 전달된다. 하지만 RNN은 은닉층이 입력층과 이전 타임 스텝(time step)의 은닉층으로부터 정보를 받는다.\n",
    "<br><br>\n",
    "인접한 타임 스텝의 정보가 은닉층에 흐르기 때문에 네트워크가 이전 이벤트를 기억할 수 있다. 이 정보 흐름은 보통 루프(loop)로 표시한다. 그래프 표기법에서는 순환 에지(recurrent edge)라고도 하기 때문에 이구조 이름이 여기서 유래되었다.\n",
    "\n",
    "다음 그림은 단일층 RNN과 다층 RNN을 비교하여 보여준다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![figure](./figure/figure04.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![figure](./figure/figure05.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN 구조와 정보흐름을 설명하기 위해 간소하게 표현한 순환 에지를 위의 그림처럼 펼처서 나타낼 수 있다.\n",
    "<br><br>\n",
    "표준 신경망의 은닉 유닛은 입력층에 연결된 최종입력 하나만 받는다. 반면 RNN의 은닉 유닛은 2개의 다른 입력을 받는다. 입력층으로부터 받은 입력과 같은 은닉층에서 t-1 타임 스텝의 활성화 출력을 받는다.\n",
    "<br><br>\n",
    "맨 처음 $t=0$에서는 은닉 유닛이 0 또는 작은 난수로 초기화된다. $t>0$인 타임 스텝에서는 은닉 유닛이 현재 타임 스텝의 데이터 포인트 $x^(t)$와 이전 타임 스텝 $t-1$의 은닉 유닉 값 $h^{t-1}$을 입력을 받는다.\n",
    "<br><br>\n",
    "요약하면 다음과 같다.\n",
    " - layer1 : 은닉층의 출력을 $h_1^{(t)}$로 표현한다. 데이터포인트 $x^{(t)}$와 이 은닉층 이전 타임 스텝 출력 $h_1^{(t-1)}$을 입력으로 받는다.\n",
    " - layer2 : 2번째 은닉층의 $h_2^{(t)}$는 이전 층의 현재 타임 스텝 출력 $h_1^{(t)}$와 이 은닉층의 이전 타임 스텝 출력 $h_2^{(t-1)}$을 입력으로 받는다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 RNN의 활성화 출력 계산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 구체적으로 은닉층과 출력층의 실제 활성화 출력을 계산해보자. 간소하게 나타내기 위해 하나의 은닉층만 고련하지만 다층 RNN에도 동일한 개념이 적용된다.\n",
    "<br><br>\n",
    "위의 그림에서 유향 에지(directed edge)(층 사이 연결과 순환 연결)는 가중치 행렬과 연관된다. 이 가중치는 특정 시간 $t$에 종속적이지 않고 전체 시간 축에 공유된다. 단일층 RNN의 각 가중치는 다음과 같다.\n",
    "<br><br>\n",
    " - 단일층 RNN\n",
    "  - $W_{xh}$ : 입력 $x^{t}$와 은닉층 $h$ 사이의 가중치 행렬 \n",
    "  - $W_{hh}$ : 순환 에지에 연관된 가중치 행렬\n",
    "  - $W_{hy}$ : 은닉층과 출력층 사이의 가중치 행렬"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![figure](./figure/figure06.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "구현에 따라 가중치 행렬 $W_{xh}$와 $W_{hh}$를 합쳐 연결된 행렬 $W_h = [W_{xh};W_{hh}]$를 사용한다.\n",
    "<br><br>\n",
    "활성화 출력의 계산은 기본적인 다층 퍼셉트론이나 다른 피드포워드 신경망과 매우 비슷하다. 은닉층의 최종입력 $z_h$(활성화 함수를 통과한 값)는 선형조합으로 계산한다. 즉, 가중치 행렬과 대응되는 벡터를 곱해서 더한 후 절편 유닛을 더한다. ($z_h^{(t)} = W_{(xh)}x^{(t)} + W_{(hh)}x^{(t-1)} + b_n$). 그 다음 타임 스텝 $t$에서 은닉층의 활성화를 계산한다.\n",
    "<br><br>\n",
    "$$h_(t) = \\phi_{h}(z_h^{t}) = \\phi_{h}(W_{(xh)}x^{(t)} + W_{(hh)}x^{(t-1)} + b_n)$$\n",
    "<br><br>\n",
    "여기서, $b_h$는 은닉 유닛의 절편 벡터고 $\\phi_h(\\cdot)$는 은닉층의 활성화 함수이다.\n",
    "<br><br>\n",
    "가중치 행렬을 $W_h = [W_{xh};W_{hh}]$처럼 연결하면 은닉 유닛에 대한 계산 공식은 다음과 같이 바뀐다.\n",
    "<br><br>\n",
    "$$h^{(t)} = \\phi_h([W_{xh};W_{hh}]\\begin{bmatrix} x^{(t)}\\\\ h^{(t-1)}\\end{bmatrix} + b_h)$$\n",
    "<br><br>\n",
    "현재 타임 스텝에서 은닉 유닛의 활성화 출력을 계산한 후 출력 유닛의 활성화를 다음과 같이 계산한다.\n",
    "<br><br>\n",
    "$$y^{(t)} = \\phi_y(W_{hy}h^{(t)} + b_y)$$\n",
    "<br><br>\n",
    "아래 그림은 두가지 버전의 계산 공식이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![figure](./figure/figure07.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Notice] BPTT로 RNN 훈련하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN의 학습 알고리즘은 1990년대에 소개되었다. 그래디언트 공식 유도가 조금 복잡하지만 기본 아이디어는 다음과 같다. 전체 손실 $L$은 t=1에서 $t=T$까지 각 타임 스텝의 모든 손실 함수의 합이다.\n",
    "<br><br>\n",
    "$$L = \\sum_{t=1}^{T}L^{(t)}$$\n",
    "<br><br>\n",
    "타임 스텝 $t$에서의 손실은 모든 이전 타임 스텝 $1:t-1$의 은닉 유닛에 의존하기 때문에 그래디언트는 다음과 같이 계산된다.\n",
    "<br><br>\n",
    "$${{\\partial L^{(t)}}\\over \\partial W_{hh}} = {{\\partial L^{(t)}}\\over \\partial y^{(t)}} \\times {{\\partial y^{(t)}}\\over \\partial h^{(t)}} \\times (\\sum_{k=1}^{t}{{\\partial h^{(t)}}\\over \\partial h^{k}} \\times {{\\partial h^{(k)}}\\over \\partial W_{hh}})$$\n",
    "<br><br>\n",
    "여기서 ${{\\partial h^{(t)}}\\over \\partial h^{(k)}}$는 이전 타임 스텝의 곱으로 계산한다.\n",
    "<br><br>\n",
    "$${{\\partial h^{(t)}}\\over \\partial h^{(k)}} = \\prod_{i=k+1}^{t} {{\\partial h^{(i)}}\\over \\partial h^{i-1}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 긴 시퀀스 학습의 어려움"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞에서 설명한 **BPTT(BackPropagation Through Time)**는 새로운 도전 과제이다.\n",
    "<br><br>\n",
    "손실 함수의 그래디언트를 계산할 때 곱셈 항인 ${{\\partial h^{(t)}}\\over \\partial h^{(k)}}$ 때문에 소위 **그래디언트 폭주(exploding gradient)** 또는 **그래디언트 소실(vanishing gradeint)** 문제가 발생한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![figure](./figure/figure08.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "${{\\partial h^{(t)}}\\over \\partial h^{(k)}}$는 $t-k$개의 곱셈으로 이루어진다. 즉, 가중치 $w$가 $t-k$번 곱해져 $w^{t-k}$가 된다. 결국 $|w|<1$이면 $t-k$가 클 때 이 항이 매우 작아진다. 반면 순환 에지의 가중치 값이 $|w|>1$이면 $t-k$가 클 때 $w^{t-k}$가 매우 커진다. $t-k$ 값이 크다는 것은 긴 시간의 의존성을 가진다는 의미이다.\n",
    "<br><br>\n",
    "그래디언트 소실이나 폭주 문제를 해결하기 위한 방법은 $|w|=1$이 되도록 만드느 것이다. 실전에서의 이 문제의 해결책은 다음 2개이다.\n",
    "<br><br>\n",
    " - T-BPTT(Truncated BackPropagation Through Time)\n",
    " - LSTM(Long Short-Term Memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "T-BPTT는 주어진 타임 스텝 너머의 그래디언트를 버린다. T-BPTT가 그래디언트 폭주 문제를 해결할 수는 있지만, 그래디언트가 시간을 거슬러 적절하게 가중치가 업데이트될 수 있는 스텝을 제한한다.\n",
    "<br><br>\n",
    "다른 방법으로 1997년 호크라이터(Hochreiter)와 슈미트후버(Schmidhuber)가 고안한 LSTM은 그래디언트 소실 문제를 극복하여 긴 시퀀스를 성공적으로 모델링할 수 있다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 LSTM 유닛"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
